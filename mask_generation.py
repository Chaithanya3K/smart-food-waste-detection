# -*- coding: utf-8 -*-
"""mask_generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dNBxuJ8ivCxLcAlkUisCZJhKi2A8Vy5b
"""

import os
import json
import numpy as np
import cv2
from PIL import Image
from glob import glob

import os
import json
import cv2
import numpy as np

# === Folder paths ===
image_dir = '/content/drive/MyDrive/vgg'                  # folder with original images
json_dir  = '/content/drive/MyDrive/manual_annotations'   # folder with individual VIA JSON files
mask_dir  = '/content/masks1'                              # save masks in Colab runtime
os.makedirs(mask_dir, exist_ok=True)

# === Read all JSON files ===
json_files = [f for f in os.listdir(json_dir) if f.endswith('.json')]

for json_file in json_files:
    json_path = os.path.join(json_dir, json_file)
    with open(json_path, 'r') as f:
        data = json.load(f)

    for key, ann in data.items():
        if 'filename' not in ann or 'regions' not in ann:
            print(f"⚠️ Skipping invalid entry: {key}")
            continue

        filename = ann['filename']
        image_path = os.path.join(image_dir, filename)

        if not os.path.exists(image_path):
            print(f"⚠️ Image not found, skipping: {filename}")
            continue

        img = cv2.imread(image_path)
        height, width = img.shape[:2]
        mask = np.zeros((height, width), dtype=np.uint8)

        for region in ann.get('regions', []):
            shape_attr = region['shape_attributes']
            if shape_attr['name'] == 'polygon':
                all_x = shape_attr['all_points_x']
                all_y = shape_attr['all_points_y']
                points = np.array(list(zip(all_x, all_y)), dtype=np.int32)
                cv2.fillPoly(mask, [points], 255)

        mask_filename = os.path.splitext(filename)[0] + '_mask.png'
        mask_path = os.path.join(mask_dir, mask_filename)
        cv2.imwrite(mask_path, mask)

        if os.path.exists(mask_path):
            print(f"✅ Saved mask in Colab: {mask_filename}")
        else:
            print(f"❌ Failed to save: {mask_filename}")

!pip install -q segmentation-models tensorflow-addons

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Paths
IMAGE_DIR = '/content/drive/MyDrive/vgg'
MASK_DIR = '/content/masks1'

# List image-mask pairs
image_files = sorted([os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR) if f.endswith('.jpg')])
mask_files  = sorted([os.path.join(MASK_DIR, f) for f in os.listdir(MASK_DIR) if f.endswith('_mask.png')])

print(f'Total image-mask pairs: {len(image_files)}')

print("Images:", len(image_files))
print("Masks:", len(mask_files))

import random

# Pick a random index
idx = random.randint(0, len(image_files)-1)

# Load image & mask
img  = cv2.imread(image_files[idx])
img  = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR → RGB
mask = cv2.imread(mask_files[idx], cv2.IMREAD_GRAYSCALE)  # Load mask in grayscale

# Show side by side
plt.figure(figsize=(10,5))

plt.subplot(1,2,1)
plt.imshow(img)
plt.title("Image")

plt.subplot(1,2,2)
plt.imshow(mask, cmap='gray')
plt.title("Mask")

plt.show()

IMG_HEIGHT, IMG_WIDTH = 256, 256  # You can also try 128x128 if GPU is slow

def preprocess(image_path, mask_path):
    image = cv2.imread(image_path)
    image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))
    image = image / 255.0  # Normalize

    mask = cv2.imread(mask_path, 0)
    mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT))
    mask = np.expand_dims(mask, axis=-1)
    mask = mask / 255.0  # Convert to 0 and 1
    return image, mask

images = []
masks = []

for img_path, mask_path in zip(image_files, mask_files):
    img, msk = preprocess(img_path, mask_path)
    images.append(img)
    masks.append(msk)

images = np.array(images)
masks = np.array(masks)

# Split into training and validation
X_train, X_val, y_train, y_val = train_test_split(images, masks, test_size=0.2, random_state=42)

print(f"Train size: {len(X_train)}, Validation size: {len(X_val)}")

import tensorflow as tf
from tensorflow.keras import layers, models

def build_unet(input_shape=(256, 256, 3)):
    inputs = layers.Input(input_shape)

    # Encoder
    c1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)
    c1 = layers.Conv2D(64, 3, activation='relu', padding='same')(c1)
    p1 = layers.MaxPooling2D()(c1)

    c2 = layers.Conv2D(128, 3, activation='relu', padding='same')(p1)
    c2 = layers.Conv2D(128, 3, activation='relu', padding='same')(c2)
    p2 = layers.MaxPooling2D()(c2)

    c3 = layers.Conv2D(256, 3, activation='relu', padding='same')(p2)
    c3 = layers.Conv2D(256, 3, activation='relu', padding='same')(c3)
    p3 = layers.MaxPooling2D()(c3)

    # Bottleneck
    c4 = layers.Conv2D(512, 3, activation='relu', padding='same')(p3)
    c4 = layers.Conv2D(512, 3, activation='relu', padding='same')(c4)

    # Decoder
    u5 = layers.UpSampling2D()(c4)
    u5 = layers.Concatenate()([u5, c3])
    c5 = layers.Conv2D(256, 3, activation='relu', padding='same')(u5)
    c5 = layers.Conv2D(256, 3, activation='relu', padding='same')(c5)

    u6 = layers.UpSampling2D()(c5)
    u6 = layers.Concatenate()([u6, c2])
    c6 = layers.Conv2D(128, 3, activation='relu', padding='same')(u6)
    c6 = layers.Conv2D(128, 3, activation='relu', padding='same')(c6)

    u7 = layers.UpSampling2D()(c6)
    u7 = layers.Concatenate()([u7, c1])
    c7 = layers.Conv2D(64, 3, activation='relu', padding='same')(u7)
    c7 = layers.Conv2D(64, 3, activation='relu', padding='same')(c7)

    outputs = layers.Conv2D(1, 1, activation='sigmoid')(c7)

    model = models.Model(inputs, outputs)
    return model

model = build_unet()
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    batch_size=8,
    epochs=10
)

plt.plot(history.history['accuracy'], label='train acc')
plt.plot(history.history['val_accuracy'], label='val acc')
plt.legend()
plt.title('Accuracy')
plt.show()

plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.legend()
plt.title('Loss')
plt.show()

model.save('/content/unet_food_segmentation.h5')

import tensorflow as tf

def iou_metric(y_true, y_pred, smooth=1e-6):
    y_pred = tf.cast(y_pred > 0.5, tf.float32)
    y_true = tf.cast(y_true, tf.float32)
    intersection = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection
    return (intersection + smooth) / (union + smooth)

model = build_unet()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy', iou_metric])

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    batch_size=8,
    epochs=10
)

