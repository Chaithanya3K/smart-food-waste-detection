# -*- coding: utf-8 -*-
"""jai_ganesha.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NyywWPJZIsUkp3W1kne3JwTgwBFvin_M
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import cv2
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from skimage.feature import local_binary_pattern

def load_images_from_folder(folder_path, image_size=(128, 128)):
    images = []
    for filename in os.listdir(folder_path):
        img = cv2.imread(os.path.join(folder_path, filename))
        if img is not None:
            img = cv2.resize(img, image_size)  # Resize to a consistent size
            img = img / 255.0  # Normalize to range 0-1
            images.append(img)
    return np.array(images)

# Load train, validation, and test images
train_images = load_images_from_folder('/content/drive/MyDrive/extracted_images_train')
val_images = load_images_from_folder('/content/drive/MyDrive/extracted_images_valid')
test_images = load_images_from_folder('/content/drive/MyDrive/extracted_images_test')

#not runed
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Apply to training images
# Get the augmented images using __next__()
train_images_augmented = next(datagen.flow(train_images, batch_size=len(train_images), shuffle=False))
# or

print(train_images.shape)

print(test_images.shape)

print(val_images.shape)

import os
import cv2
import numpy as np
from skimage.feature import local_binary_pattern
import pandas as pd

# Function to create a folder if it does not exist
def ensure_folder_exists(folder_path):
    if not os.path.exists(folder_path):
        print(f"Folder not found: {folder_path}. Creating it now.")
        os.makedirs(folder_path)

# Function to load images from a specified folder
def load_images_from_folder(folder_path):
    ensure_folder_exists(folder_path)  # Ensure folder exists
    images = []
    filenames = []
    for filename in os.listdir(folder_path):
        img = cv2.imread(os.path.join(folder_path, filename))
        if img is not None:
            images.append(img)
            filenames.append(filename)
    return images, filenames

# Color feature extraction (example using color histogram)
def extract_color_features(image, bins=(8, 8, 8)):
    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    hist = cv2.calcHist([hsv_image], [0, 1, 2], None, bins, [0, 256, 0, 256, 0, 256])
    cv2.normalize(hist, hist)
    return hist.flatten()

# Texture feature extraction using LBP
def extract_texture_features(image, radius=1):
    n_points = 8 * radius
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    lbp = local_binary_pattern(gray_image, n_points, radius, method="uniform")
    (hist, _) = np.histogram(lbp.ravel(),
                             bins=np.arange(0, n_points + 3),
                             range=(0, n_points + 2))
    hist = hist.astype("float")
    hist /= (hist.sum() + 1e-6)
    return hist

# Shape feature extraction (example using contours)
def extract_shape_features(image):
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if contours:
        cnt = max(contours, key=cv2.contourArea)
        area = cv2.contourArea(cnt)
        perimeter = cv2.arcLength(cnt, True)
        return np.array([area, perimeter])
    else:
        return np.array([0, 0])

# Function to extract all features from an image
def extract_features(image):
    color_features = extract_color_features(image)
    texture_features = extract_texture_features(image)
    shape_features = extract_shape_features(image)
    return np.concatenate([color_features, texture_features, shape_features])

# Save features to a CSV file
def save_features_to_csv(features, filenames, save_path):
    # Combine filenames and features into a DataFrame
    feature_data = pd.DataFrame(features)
    feature_data.insert(0, "Filename", filenames)  # Add filenames as the first column

    if not os.path.exists(save_path):  # If the file does not exist, create it
        print(f"File not found: {save_path}. Creating it now.")
        feature_data.to_csv(save_path, index=False)
    else:  # If the file exists, append new data to it
        print(f"File found: {save_path}. Appending data.")
        feature_data.to_csv(save_path, mode='a', index=False, header=False)

# Main function
if __name__ == "__main__":
    # Paths to image folders
    train_folder = '/content/drive/MyDrive/extracted_images_train'
    val_folder = '/content/drive/MyDrive/extracted_images_valid'
    test_folder = '/content/drive/MyDrive/extracted_images_test'
    # Paths to save CSV files in the user's Documents folder
    train_features_file = 'C:\\Users\\hp\\Documents\\train_combined_features.csv'
    val_features_file = 'C:\\Users\\hp\\Documents\\val_combined_features.csv'
    test_features_file = 'C:\\Users\\hp\\Documents\\test_combined_features.csv'

    # Ensure folders exist
    ensure_folder_exists(train_folder)
    ensure_folder_exists(val_folder)
    ensure_folder_exists(test_folder)

    # Load images
    train_images, train_filenames = load_images_from_folder(train_folder)
    val_images, val_filenames = load_images_from_folder(val_folder)
    test_images, test_filenames = load_images_from_folder(test_folder)

    # Extract features for all images in train, val, and test datasets
    print("Extracting features for training data...")
    train_features = [extract_features(img) for img in train_images]

    print("Extracting features for validation data...")
    val_features = [extract_features(img) for img in val_images]

    print("Extracting features for testing data...")
    test_features = [extract_features(img) for img in test_images]

    # Save the combined feature data to CSV files
    print("Saving training features...")
    save_features_to_csv(train_features, train_filenames, train_features_file)

    print("Saving validation features...")
    save_features_to_csv(val_features, val_filenames, val_features_file)

    print("Saving testing features...")
    save_features_to_csv(test_features, test_filenames, test_features_file)

    print("Feature extraction and saving completed!")

import pandas as pd

# Paths to the saved CSV files
train_features_file = r"C:\Users\hp\Documents\train_combined_features.csv"
val_features_file = r"C:\Users\hp\Documents\val_combined_features.csv"
test_features_file = r"C:\Users\hp\Documents\test_combined_features.csv"

# Function to load and print first 5 rows of a CSV file
def print_first_5_rows(file_path, label):
    try:
        df = pd.read_csv(file_path)
        print(f"\nFirst 5 rows of {label}:")
        print(df.head(5))
    except Exception as e:
        print(f"Error reading file {file_path}: {e}")

# Print first 5 rows for each file
print_first_5_rows(train_features_file, "Training Features")
print_first_5_rows(val_features_file, "Validation Features")
print_first_5_rows(test_features_file, "Testing Features")

import matplotlib.pyplot as plt
import numpy as np
import os
import cv2

# Feature extraction functions (similar to before)
def extract_color_features(image):
    color_hist = [cv2.calcHist([image], [i], None, [256], [0, 256]).flatten() for i in range(3)]
    return np.concatenate(color_hist)

def extract_texture_features(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    return np.histogram(gray.ravel(), bins=10, range=(0, 256))[0]

def extract_shape_features(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    areas = np.array([cv2.contourArea(cnt) for cnt in contours])
    perimeters = np.array([cv2.arcLength(cnt, True) for cnt in contours])
    return np.array([areas.sum(), perimeters.sum()])

# Helper function to load images
def load_images_from_folder(folder):
    images = []
    for filename in os.listdir(folder):
        filepath = os.path.join(folder, filename)
        if os.path.isfile(filepath):
            image = cv2.imread(filepath)
            if image is not None:
                images.append(image)
    return images

# Combined plotting function
def plot_combined_histograms(color_features, texture_features, shape_features, dataset_name):
    # Create subplots
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    fig.suptitle(f'{dataset_name} Features', fontsize=16)

    # Plot color features
    axes[0].hist(color_features.ravel(), bins=50, color='blue', alpha=0.7)
    axes[0].set_title('Color Features (Histogram)')
    axes[0].set_xlabel('Bin')
    axes[0].set_ylabel('Frequency')

    # Plot texture features
    axes[1].bar(range(len(texture_features)), texture_features, color='gray')
    axes[1].set_title('Texture Features (LBP Histogram)')
    axes[1].set_xlabel('Pattern')
    axes[1].set_ylabel('Frequency')

    # Plot shape features
    axes[2].bar(['Area', 'Perimeter'], shape_features, color=['blue', 'green'])
    axes[2].set_title('Shape Features')
    axes[2].set_ylabel('Value')

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

# Processing datasets
def process_and_plot(folder, dataset_name):
    images = load_images_from_folder(folder)

    if not images:
        print(f"No images found in {folder}.")
        return

    # Extract features
    color_features = [extract_color_features(img) for img in images]
    texture_features = [extract_texture_features(img) for img in images]
    shape_features = [extract_shape_features(img) for img in images]

    # Aggregate features for plotting
    color_features = np.concatenate(color_features)
    texture_features = np.sum(texture_features, axis=0)
    shape_features = np.sum(shape_features, axis=0)

    # Plot combined histograms
    plot_combined_histograms(color_features, texture_features, shape_features, dataset_name)

# Paths
train_folder = '/content/drive/MyDrive/extracted_images_train'
val_folder = '/content/drive/MyDrive/extracted_images_valid'
test_folder = '/content/drive/MyDrive/extracted_images_test'

# Execute for each dataset
process_and_plot(train_folder, "Training")
process_and_plot(val_folder, "Validation")
process_and_plot(test_folder, "Testing")

import pandas as pd

# Define function to process and print the entire data
def process_and_plot(file_path, label):
    # Load the data from the provided CSV file
    df = pd.read_csv(file_path)

    # Print the entire data
    print(f"\n{label} Data:")
    print(df)  # This will print the entire data

# Path to your specific training file
train_features_file = 'C:\\Users\\hp\\Documents\\train_combined_features.csv'

# Process and print the entire training data
process_and_plot(train_features_file, "Training")

import pandas as pd
import numpy as np

# Load the original training data (train_combined_features.csv)
train_data = pd.read_csv('C:\\Users\\hp\\Documents\\train_combined_features.csv')

# Function to calculate area of food waste (can be adjusted based on actual feature data)
def calculate_area(features):
    # Here, we assume that area can be inferred from certain features (you should adjust this logic).
    # This is just an example and should be adapted to your specific data.
    features = np.array(features.split(',')).astype(float)  # Assuming features are comma-separated
    area = np.sum(features)  # This is a placeholder for how you might calculate the area
    return area

# Example of classifying food as waste or not waste based on some threshold on features
def classify_food(features):
    # A simple threshold for waste detection: adjust based on your data and classification logic
    features = np.array(features.split(',')).astype(float)
    if np.sum(features) > 0.6:  # This threshold can be tuned based on your data
        return 'Waste'
    else:
        return 'Non-Waste'

# Apply area calculation and classification
train_data['Area'] = train_data['Features'].apply(calculate_area)
train_data['Class'] = train_data['Features'].apply(classify_food)

# Save the new data with 'Area' and 'Class' to a new CSV
train_data[['Area', 'Class']].to_csv('C:\\Users\\hp\\Documents\\food_waste_area_class.csv', index=False)

print("New CSV file with Area and Class has been created.")

import pandas as pd
import numpy as np

# Load the original training data (train_combined_features.csv)
train_data = pd.read_csv('C:\\Users\\hp\\Documents\\train_combined_features.csv')

# Function to calculate area of food waste (can be adjusted based on actual feature data)
def calculate_area(features):
    # Here, we assume that area can be inferred from certain features (you should adjust this logic).
    # This is just an example and should be adapted to your specific data.
    features = np.array(features.split(',')).astype(float)  # Assuming features are comma-separated
    area = np.sum(features)  # This is a placeholder for how you might calculate the area
    return area

# Example of classifying food as waste or not waste based on some threshold on features
def classify_food(features):
    # A simple threshold for waste detection: adjust based on your data and classification logic
    features = np.array(features.split(',')).astype(float)
    if np.sum(features) > 0.5:  # This threshold can be tuned based on your data
        return 'Waste'
    else:
        return 'Non-Waste'

# Apply area calculation and classification
train_data['Area'] = train_data['Features'].apply(calculate_area)
train_data['Class'] = train_data['Features'].apply(classify_food)

# Save the new data with 'Area' and 'Class' to a new CSV
train_data[['Area', 'Class']].to_csv('C:\\Users\\hp\\Documents\\food_waste_area_class.csv', index=False)

# Print the first 5 rows of the new data
print("First 5 rows of the data with Area and Class:")
print(train_data[['Area', 'Class']].head())

from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# Load the training dataset
train_data = pd.read_csv('C:\\Users\\hp\\Documents\\food_waste_area_class.csv')

# Features (using 'Area' as the feature) and target for training
X_train = train_data[['Area']]  # Feature (Area)
y_train = train_data['Class']   # Target (Class)

# Initialize the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model on the training data
rf_model.fit(X_train, y_train)

# Print confirmation
print("Model training complete.")

#wait





